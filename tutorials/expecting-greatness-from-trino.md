# Expecting Greatness from Trino

## Landing Tables setup

### MinIO storage setup

By default, this compose setup has two CSV files available for consumption in MinIO (S3). You can imagine that these were generated by
an streaming or batch job that occured upstream. I have provided you the `CREATE TABLE` statements that include the schema needed for
Trino to parse these CSV files. CSV files are currently only supported in the Hive connector at the time of writing this tutorial. So
you will start by using the Hive connector to read this data, but will optimize the table by using the Iceberg connector when creating
the _structure_ and _consume_ tables.

To open these files, navigate to [http://localhost:9001](http://localhost:9001/). Open the `lakehouse` bucket.

There are two files located at `/lakehouse/pokemon/csv/` and `/lakehouse/pokemon_spawns/csv/` which contain a dimension table with 
Pokemon info and stats, and a fact table which contains Pokemon Go Spawn data. Our goal is to first transform these csv files into
_structure_ tables that can be reused in the optimal format to create various _consume_ tables.

No time to waste! Gotta Query 'em all!

### Executing the queries on Trino

```
CREATE SCHEMA hive.lakehouse
WITH (location = 's3a://lakehouse/');

CREATE TABLE hive.lakehouse.pokemon_csv(
  "Number" VARCHAR,
  "Name" VARCHAR,
  "Type 1" VARCHAR,
  "Type 2" VARCHAR,
  "Abilities" VARCHAR,
  "HP" VARCHAR,
  "Att" VARCHAR,
  "Def" VARCHAR,
  "Spa" VARCHAR,
  "Spd" VARCHAR,
  "Spe" VARCHAR,
  "BST" VARCHAR,
  "Mean" VARCHAR,
  "Standard Deviation" VARCHAR,
  "Generation" VARCHAR,
  "Experience type" VARCHAR,
  "Experience to level 100" VARCHAR,
  "Final Evolution" VARCHAR,
  "Catch Rate" VARCHAR,
  "Legendary" VARCHAR,
  "Mega Evolution" VARCHAR,
  "Alolan Form" VARCHAR,
  "Galarian Form" VARCHAR,
  "Against Normal" VARCHAR,
  "Against Fire" VARCHAR,
  "Against Water" VARCHAR,
  "Against Electric" VARCHAR,
  "Against Grass" VARCHAR,
  "Against Ice" VARCHAR,
  "Against Fighting" VARCHAR,
  "Against Poison" VARCHAR,
  "Against Ground" VARCHAR,
  "Against Flying" VARCHAR,
  "Against Psychic" VARCHAR,
  "Against Bug" VARCHAR,
  "Against Rock" VARCHAR,
  "Against Ghost" VARCHAR,
  "Against Dragon" VARCHAR,
  "Against Dark" VARCHAR,
  "Against Steel" VARCHAR,
  "Against Fairy" VARCHAR,
  "Height" VARCHAR,
  "Weight" VARCHAR,
  "BMI" VARCHAR
)
WITH (
  format = 'CSV',
  external_location = 's3a://lakehouse/pokemon/csv',
  skip_header_line_count=1
);

CREATE TABLE hive.lakehouse.pokemon_spawns_csv(
  "s2_id" VARCHAR,
  "s2_token" VARCHAR,
  "num" VARCHAR,
  "name" VARCHAR,
  "lat" VARCHAR,
  "long" VARCHAR,
  "encounter_ms" VARCHAR,
  "disappear_ms" VARCHAR
)
WITH (
  format = 'CSV',
  external_location = 's3a://lakehouse/pokemon_spawns/csv',
  skip_header_line_count=1
);

```

Validate everything is working by running  if the data can be read.

```
SELECT * FROM hive.lakehouse.pokemon_csv LIMIT 10;
SELECT * FROM hive.lakehouse.pokemon_spawns_csv LIMIT 10;
```

## Structure table setup

Now create the structure tables using a `CREATE TABLE AS` statement.

```
CREATE TABLE iceberg.lakehouse.pokemon 
WITH (
  format = 'ORC',
  location = 's3a://lakehouse/pokemon/orc'
) AS
SELECT
  CAST(number AS INTEGER) AS number,
  name,
  "Type 1" AS type1,
  "Type 2" AS type2,
  CAST(json_parse(replace(replace(Abilities, '''s', 's'), '''', '"')) AS ARRAY(VARCHAR)) AS abilities,
  CAST(hp AS INTEGER) AS hp,
  CAST(att AS INTEGER) AS att,
  CAST(def AS INTEGER) AS def,
  CAST(spa AS INTEGER) AS spa,
  CAST(spd AS INTEGER) AS spd,
  CAST(spe AS INTEGER) AS spe,
  CAST(bst AS INTEGER) AS bst,
  CAST(mean AS DOUBLE) AS mean,
  CAST("Standard Deviation" AS DOUBLE) AS std_dev,
  CAST(generation AS DOUBLE) AS generation,
  "Experience type" AS experience_type,
  CAST("Experience to level 100" AS BIGINT) AS experience_to_lvl_100,
  CAST(CAST("Final Evolution" AS DOUBLE) AS BOOLEAN) AS final_evolution,
  CAST("Catch Rate" AS INTEGER) AS catch_rate,
  CAST(CAST("Legendary" AS DOUBLE) AS BOOLEAN) AS legendary,
  CAST(CAST("Mega Evolution" AS DOUBLE) AS BOOLEAN) AS mega_evolution,
  CAST(CAST("Alolan Form" AS DOUBLE) AS BOOLEAN) AS alolan_form,
  CAST(CAST("Galarian Form" AS DOUBLE) AS BOOLEAN) AS galarian_form,
  CAST("Against Normal" AS DOUBLE) AS against_normal,
  CAST("Against Fire" AS DOUBLE) AS against_fire,
  CAST("Against Water" AS DOUBLE) AS against_water,
  CAST("Against Electric" AS DOUBLE) AS against_electric,
  CAST("Against Grass" AS DOUBLE) AS against_grass,
  CAST("Against Ice" AS DOUBLE) AS against_ice,
  CAST("Against Fighting" AS DOUBLE) AS against_fighting,
  CAST("Against Poison" AS DOUBLE) AS against_poison,
  CAST("Against Ground" AS DOUBLE) AS against_ground,
  CAST("Against Flying" AS DOUBLE) AS against_flying,
  CAST("Against Psychic" AS DOUBLE) AS against_psychic,
  CAST("Against Bug" AS DOUBLE) AS against_bug,
  CAST("Against Rock" AS DOUBLE) AS against_rock,
  CAST("Against Ghost" AS DOUBLE) AS against_ghost,
  CAST("Against Dragon" AS DOUBLE) AS against_dragon,
  CAST("Against Dark" AS DOUBLE) AS against_dark,
  CAST("Against Steel" AS DOUBLE) AS against_steel,
  CAST("Against Fairy" AS DOUBLE) AS against_fairy,
  CAST("Height" AS DOUBLE) AS height,
  CAST("Weight" AS DOUBLE) AS weight,
  CAST("BMI" AS DOUBLE) AS bmi
FROM hive.lakehouse.pokemon_csv;

CREATE TABLE iceberg.lakehouse.pokemon_spawns
WITH (
  format = 'ORC',
  location = 's3a://lakehouse/pokemon_spawns/orc'
) AS
SELECT
  CAST(num AS INTEGER) AS number,
  name,
  CAST(lat AS DOUBLE) AS lat,
  CAST(long AS DOUBLE) AS long,
  CAST(encounter_ms AS BIGINT) AS encounter_ms,
  CAST(disappear_ms AS BIGINT) AS disappear_ms
FROM hive.lakehouse.pokemon_spawns_csv;
```

## Consume table setup

Finally, with the two structure tables, you'll run a final join query to add context to the spawns data.

```
CREATE TABLE iceberg.lakehouse.pokemon_spawns_by_type AS
SELECT s.*, p.type1, p.type2, p.legendary
FROM  iceberg.lakehouse.pokemon p 
 JOIN iceberg.lakehouse.pokemon_spawns s 
 ON p.number = s.number AND p.mega_evolution = FALSE;
```

## Setup for Great Expectations

Great expectations is installed on the lake edge node. So you'll want to start off by logging in there.

```
docker exec -it trino-datalake_lake-edge_1 /bin/bash
```

### Initialize the Great Expectations directory

Under the `/etc` directory, there is a great_expectations folder that you will have access to overwrite.
Move to the `/etc` directory, and then run the following command:

```
great_expectations init
```

After running this command, you'll see the output below, answer 'Y' to proceed:

```
  ___              _     ___                  _        _   _
 / __|_ _ ___ __ _| |_  | __|_ ___ __  ___ __| |_ __ _| |_(_)___ _ _  ___
| (_ | '_/ -_) _` |  _| | _|\ \ / '_ \/ -_) _|  _/ _` |  _| / _ \ ' \(_-<
 \___|_| \___\__,_|\__| |___/_\_\ .__/\___\__|\__\__,_|\__|_\___/_||_/__/
                                |_|
             ~ Always know what to expect from your data ~

Let's create a new Data Context to hold your project configuration.

Great Expectations will create a new directory with the following structure:

    great_expectations
    |-- great_expectations.yml
    |-- ...

OK to proceed? [Y/n]: Y
```

### Set up a data source

Create a datasource for both `iceberg` and `hive` catalogs. This is needed as great expectations does not
support the notion of a three tiered containment hierarchy (e.g. catalog.schema.table in Trino). This doesn't
pose an issue as validations occur distinctly for each resulting table in various catalogs. This does mean
that you'll need to create a different data source for each catalog in Trino for now.

```
great_expectations datasource new
```

* Select `2. Relational database (SQL)`

* Select `6. other - Do you have a working SQLAlchemy connection string?`

At this point you will get output like this:

```
[I 20:00:41.111 NotebookApp] Jupyter Notebook 6.4.12 is running at:
[I 20:00:41.111 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 20:00:41.117 NotebookApp] No web browser found: could not locate runnable browser.
[C 20:00:41.117 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/bunbun/.local/share/jupyter/runtime/nbserver-58-open.html
    Or copy and paste one of these URLs:
        http://lake-edge:8888/?token=<GENERATED_TOKEN>
     or http://127.0.0.1:8888/?token=<GENERATED_TOKEN>
```

To edit Great Expectations files, copy the url with the '127.0.0.1' address in it, for example,
http://127.0.0.1:8888/?token=<GENERATED_TOKEN> from the output above in your local web browser.

In the web browser, you'll be brought to a jupyter file tree view. Click on the 'datasource_new.ipynb'
to configure the `trino_iceberg` and `trino_hive` data sources with the following fields. Be sure
to run all the steps sequentially in the notebook to set up all the context and other objects for
Great Expectations.

* datasource_name: trino_iceberg
* connection_string: trino://trino@trino-coordinator:8080/iceberg/lakehouse

* datasource_name: trino_hive
* connection_string: trino://trino@trino-coordinator:8080/hive/lakehouse


### Set up a test suite

Create a test suite for all of the tables generated that need validation.

```
great_expectations suite new
```
Select the following options

* Select `2. Interactively, with a sample batch of data`
* Select the option with `lakehouse.pokemon_csv`

Use the suite name: `lakehouse.pokemon_csv.suite`, and 
At this point you will get ouput that brings up the notebook url again. Continue to choose the url 
with the '127.0.0.1' address in it.

In the web browser, you'll be brought to a jupyter file tree view. Click on the 
'edit_lakehouse.pokemon_csv.suite.ipynb'. Once you've opened this, copy the expectations from below
for the `pokemon_csv` dataset. Once those are there, you can start running the steps in the notebook
Be sure to run all the steps sequentially in the notebook to set up all the context and other 
objects for Great Expectations.

Create the rest of the suites for the the `pokemon`, `pokemon_spawns_csv`, `pokemon_spawns`, and the
`pokemon_spawns_by_type` table in the same format.

### Adding expectations

Add the following expectations code in the steps above for the given tables in Trino.

`pokemon_csv`

```
validator.expect_table_row_count_to_equal(1032)
validator.expect_table_columns_to_match_ordered_list(column_list=[
          "number",
          "name",
          "type 1",
          "type 2",
          "abilities",
          "hp",
          "att",
          "def",
          "spa",
          "spd",
          "spe",
          "bst",
          "mean",
          "standard deviation",
          "generation",
          "experience type",
          "experience to level 100",
          "final evolution",
          "catch rate",
          "legendary",
          "mega evolution",
          "alolan form",
          "galarian form",
          "against normal",
          "against fire",
          "against water",
          "against electric",
          "against grass",
          "against ice",
          "against fighting",
          "against poison",
          "against ground",
          "against flying",
          "against psychic",
          "against bug",
          "against rock",
          "against ghost",
          "against dragon",
          "against dark",
          "against steel",
          "against fairy",
          "height",
          "weight",
          "bmi"
        ])
validator.expect_column_values_to_be_of_type(column="number", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="name", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="type 1", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="type 2", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="abilities", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="hp", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="att", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="def", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="spa", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="spd", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="spe", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="bst", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="mean", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="standard deviation", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="generation", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="experience type", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="experience to level 100", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="final evolution", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="catch rate", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="legendary", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="mega evolution", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="alolan form", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="galarian form", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against normal", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against fire", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against water", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against electric", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against grass", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against ice", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against fighting", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against poison", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against ground", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against flying", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against psychic", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against bug", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against rock", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against ghost", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against dragon", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against dark", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against steel", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="against fairy", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="height", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="weight", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="bmi", type_="VARCHAR")
```

`pokemon_spawns_csv`

this looks similar to the `pokemon_csv` expectations, except we don't validate the number of rows,
since this will be different in future batchs.

```
validator.expect_table_columns_to_match_ordered_list(column_list=[
          "s2_id",
          "s2_token",
          "num",
          "name",
          "lat",
          "long",
          "encounter_ms",
          "disappear_ms"
        ])
validator.expect_column_values_to_be_of_type(column="s2_id", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="s2_token", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="num", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="name", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="lat", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="long", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="encounter_ms", type_="VARCHAR")
validator.expect_column_values_to_be_of_type(column="disappear_ms", type_="VARCHAR")
```

`pokemon`

```
validator.expect_table_row_count_to_equal(1032)
validator.expect_column_values_to_be_between(column="number", min_value=1, max_value=898)
validator.expect_column_values_to_be_of_type(column="abilities", type_="ARRAY")
validator.expect_column_values_to_be_of_type(column="hp", type_="INTEGER")
validator.expect_column_values_to_be_of_type(column="final_evolution", type_="BOOLEAN")
```

`pokemon_spawns`

```
validator.expect_column_values_to_be_between(column="number", min_value=1, max_value=898)
validator.expect_column_values_to_be_between(column="lat", min_value=-90, max_value=90)
validator.expect_column_values_to_be_between(column="long", min_value=-180, max_value=180)
validator.expect_column_values_to_be_of_type(column="number", type_="INTEGER")
validator.expect_column_values_to_be_of_type(column="lat", type_="DOUBLE")
validator.expect_column_values_to_be_of_type(column="encounter_ms", type_="BIGINT")
```

`pokemon_spawns_by_type`

```
validator.expect_table_row_count_to_equal(369231)
validator.expect_column_values_to_not_be_null(column="number")
validator.expect_column_values_to_not_be_null(column="name")
validator.expect_column_values_to_not_be_null(column="lat")
validator.expect_column_values_to_not_be_null(column="long")
validator.expect_column_values_to_not_be_null(column="encounter_ms")
validator.expect_column_values_to_not_be_null(column="disappear_ms")
validator.expect_column_values_to_not_be_null(column="type1")
validator.expect_table_columns_to_match_ordered_list(column_list=[
          "number",
          "name",
          "lat",
          "long",
          "encounter_ms",
          "disappear_ms",
          "type1",
          "type2",
          "legendary"
        ])
```

Once all of the expectations have been added, you can create a checkpoint.

### Check out local documentation

Now that you have created your expectations, you can look at the generated documentation.
From the jupyter tree view, navigate to, 
`data_docs/local_site/validations/lakehouse/<table>/suite/__none__/<timestamp_dir>/<generated_id>.html`.
You can look at these for the documentation generated for each table.

I hope this gave you a good sense of how powerful this library can be and how it can validate the
state of your data without having to manually check it or doing sporadic guess work.

Take care and happy querying.
